{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RCNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishaanharry/UCSD-DML-image-segmentation/blob/main/RCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKAjgUA6CO_M"
      },
      "source": [
        "# Imports\n",
        "# testing\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from time import time\n",
        "import torchvision.transforms as T\n",
        "import torch.utils.data\n",
        "from natsort import natsorted\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib import patches\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fV7qxWnqCQCJ",
        "outputId": "a2aaec72-9d76-4574-9ac8-273adf01c1ec"
      },
      "source": [
        "# Check if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    print(\"The code will run on GPU.\")\n",
        "else:\n",
        "    print(\"The code will run on CPU. Go to Edit->Notebook Settings and choose GPU as the hardware accelerator\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The code will run on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kFl7Cj0CZ_7",
        "outputId": "8654d9ba-56bc-44b1-ad26-0e3674a8b48e"
      },
      "source": [
        "from google.colab import drive\n",
        "# This will prompt for authorization.\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "!ls '/content/drive/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMhZ3cM9CRTT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "e42e712b-df5b-47fe-8e4e-0f30e9d58b4f"
      },
      "source": [
        "# Import fasterrcnn model, and other libraries\n",
        "\n",
        "# /content/drive/My Drive/Colab Notebooks/02456-deep-learning-with-PyTorch-4_Mini_Project\n",
        "import sys\n",
        "drive_path = '/content/drive/MyDrive/warwick_qu_dataset_released_2016_07_08/Warwick QU Dataset (Released 2016_07_08)/detection'\n",
        "sys.path.insert(0, drive_path)\n",
        "module_names = [ f.name for f in os.scandir(drive_path)]\n",
        "# module_names\n",
        "for i in module_names:\n",
        "  if 'README' in i:\n",
        "    continue\n",
        "  i = i.replace('.py', '') \n",
        "  __import__(i)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-16fa7fcd1d86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdrive_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/warwick_qu_dataset_released_2016_07_08/Warwick QU Dataset (Released 2016_07_08)/detection'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrive_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodule_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrive_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# module_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/warwick_qu_dataset_released_2016_07_08/Warwick QU Dataset (Released 2016_07_08)/detection'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv2UK5YuC-Sk"
      },
      "source": [
        "\n",
        "class WarwickCellDataset(object):\n",
        "    def __init__(self, root, transforms=None): # transforms\n",
        "        self.root = root\n",
        "        # self.transforms = transforms\n",
        "        self.transforms=[]\n",
        "        if transforms!=None:\n",
        "          self.transforms.append(transforms)\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.imgs = list(natsorted(os.listdir(os.path.join(root, \"image\"))))\n",
        "        print('imgs file names:', self.imgs)\n",
        "        self.masks = list(natsorted(os.listdir(os.path.join(root, \"mask\"))))\n",
        "        print('masks file names:', self.masks)\n",
        "        # images = []\n",
        "        # masks = []\n",
        "        # image_names = [f.name for f in os.scandir(root)]\n",
        "        # image_names = natsorted(image_names)\n",
        "        # for image_name in image_names:\n",
        "        #   if image_name.endswith('_anno.bmp'):\n",
        "        #     masks.append(image_name)\n",
        "        #   else:\n",
        "        #     images.append(image_name)\n",
        "        # print(images)\n",
        "        # print(masks)\n",
        "        # print('Length images:', len(images))\n",
        "        # print('Length masks:', len(masks))\n",
        "        # self.imgs = images\n",
        "        # self.masks = masks\n",
        "        # print('Length images:', len(self.imgs))\n",
        "        # print('Length masks:', len(self.masks))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # idx sometimes goes over the nr of training images, add logic to keep it lower\n",
        "        if idx >= 80:\n",
        "          idx = np.random.randint(80, size=1)[0]\n",
        "        # print(idx)\n",
        "        # load images ad masks\n",
        "        # print('idx:', idx)\n",
        "        img_path = os.path.join(self.root, \"image\", self.imgs[idx])\n",
        "        # print('img_path', self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"mask\", self.masks[idx])\n",
        "        # print('mask_path', self.masks[idx])\n",
        "        # print('img_path name:', self.imgs[idx])\n",
        "        # img_path = os.path.join(self.root, self.imgs[idx])\n",
        "        # print('mask_path name:', self.masks[idx])\n",
        "        # mask_path = os.path.join(self.root, self.masks[idx])\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        # note that we haven't converted the mask to RGB,\n",
        "        # because each color corresponds to a different instance\n",
        "        # with 0 being background\n",
        "        mask = Image.open(mask_path)\n",
        "        # convert the PIL Image into a numpy array\n",
        "        mask = np.array(mask)\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = np.unique(mask)\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        num_objs = len(obj_ids)\n",
        "        # print(num_objs)\n",
        "        boxes = []\n",
        "        for i in range(num_objs):\n",
        "          pos = np.where(masks[i])\n",
        "          xmin = np.min(pos[1])\n",
        "          xmax = np.max(pos[1])\n",
        "          ymin = np.min(pos[0])\n",
        "          ymax = np.max(pos[0])\n",
        "          # Check if area is larger than a threshold\n",
        "          A = abs((xmax-xmin) * (ymax-ymin)) \n",
        "          # print(A)\n",
        "          if A < 5:\n",
        "            print('Nr before deletion:', num_objs)\n",
        "            obj_ids=np.delete(obj_ids, [i])\n",
        "            # print('Area smaller than 5! Box coordinates:', [xmin, ymin, xmax, ymax])\n",
        "            print('Nr after deletion:', len(obj_ids))\n",
        "            continue\n",
        "            # xmax=xmax+5 \n",
        "            # ymax=ymax+5\n",
        "\n",
        "          boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        # print('nr boxes is equal to nr ids:', len(boxes)==len(obj_ids))\n",
        "        num_objs = len(obj_ids)\n",
        "        # convert everything into a torch.Tensor\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        for i in self.transforms:\n",
        "          img = i(img)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"labels\"] = labels # Not sure if this is needed\n",
        "        \n",
        "        # trans = transforms.ToTensor()\n",
        "        # img = trans(img)\n",
        "        # transform = []\n",
        "        # transform.append(transforms.RandomHorizontalFlip(0.5))\n",
        "        # transform.append(transforms.ToTensor())\n",
        "        # transform = Compose(transform)\n",
        "        # img, target = transform(img, target)\n",
        "\n",
        "        # if self.transforms is not None:\n",
        "        #   # for t in self.transforms:\n",
        "        #   #   img = t(img)\n",
        "        #   #   target = t(target)\n",
        "        #     # print(img.size)\n",
        "        #     img, target = self.transforms(img, target)\n",
        "\n",
        "        return img.double(), target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiVMy2D2CVG5"
      },
      "source": [
        "root_train = '/content/drive/MyDrive/warwick_qu_dataset_released_2016_07_08/Warwick QU Dataset (Released 2016_07_08)/train'\n",
        "dataset_train = WarwickCellDataset(root_train, transforms=torchvision.transforms.ToTensor()) # get_transform(train=True)\n",
        "dataset_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao7usj-ME8q2"
      },
      "source": [
        "# Define data loader for training set\n",
        "data_loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=2, shuffle=True,collate_fn=lambda x:list(zip(*x)))\n",
        "print(len(data_loader_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vi0f_oIoFMuI"
      },
      "source": [
        "# View some images and their bbox's in the training set.\n",
        "\n",
        "images,labels=next(iter(data_loader_train))\n",
        "\n",
        "def view(images,labels,n=2,std=1,mean=0):\n",
        "    figure = plt.figure(figsize=(15,10))\n",
        "    images=list(images)\n",
        "    labels=list(labels)\n",
        "    for i in range(n):\n",
        "        out=torchvision.utils.make_grid(images[i])\n",
        "        inp=out.cpu().numpy().transpose((1,2,0))\n",
        "        inp=np.array(std)*inp+np.array(mean)\n",
        "        inp=np.clip(inp,0,1)  \n",
        "        ax = figure.add_subplot(2,2, i + 1)\n",
        "        ax.imshow(images[i].cpu().numpy().transpose((1,2,0)))\n",
        "        l=labels[i]['boxes'].cpu().numpy()\n",
        "        l[:,2]=l[:,2]-l[:,0]\n",
        "        l[:,3]=l[:,3]-l[:,1]\n",
        "        for j in range(len(l)):\n",
        "            ax.add_patch(patches.Rectangle((l[j][0],l[j][1]),l[j][2],l[j][3],linewidth=1.5,edgecolor='r',facecolor='none')) \n",
        "\n",
        "view(images=images,labels=labels,n=2,std=1,mean=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "IH8isIV46Xez"
      },
      "source": [
        "#@title\n",
        "# import torchvision\n",
        "# from torchvision.models.detection import FasterRCNN\n",
        "# from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "# # load a pre-trained model for classification and return\n",
        "# # only the features\n",
        "# backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
        "# # FasterRCNN needs to know the number of\n",
        "# # output channels in a backbone. For mobilenet_v2, it's 1280\n",
        "# # so we need to add it here\n",
        "# backbone.out_channels = 1280\n",
        "\n",
        "# # let's make the RPN generate 5 x 3 anchors per spatial\n",
        "# # location, with 5 different sizes and 3 different aspect\n",
        "# # ratios. We have a Tuple[Tuple[int]] because each feature\n",
        "# # map could potentially have different sizes and\n",
        "# # aspect ratios\n",
        "# anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
        "#                                    aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "\n",
        "# # let's define what are the feature maps that we will\n",
        "# # use to perform the region of interest cropping, as well as\n",
        "# # the size of the crop after rescaling.\n",
        "# # if your backbone returns a Tensor, featmap_names is expected to\n",
        "# # be [0]. More generally, the backbone should return an\n",
        "# # OrderedDict[Tensor], and in featmap_names you can choose which\n",
        "# # feature maps to use.\n",
        "# roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
        "#                                                 output_size=7,\n",
        "#                                                 sampling_ratio=2)\n",
        "\n",
        "\n",
        "\n",
        "# # put the pieces together inside a FasterRCNN model\n",
        "# model = FasterRCNN(backbone,\n",
        "#                    num_classes=2,\n",
        "#                    rpn_anchor_generator=anchor_generator,\n",
        "#                    box_roi_pool=roi_pooler)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "PcL9DQXQGl7C"
      },
      "source": [
        "#@title\n",
        "# import torchvision\n",
        "# from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "# from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "# num_classes = 2  # 1 class (cell) + background = 2 classes\n",
        "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) # use the torch model, pre-trained on coco dataset if set to True\n",
        "# in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "# model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "# # now get the number of input features for the mask classifier\n",
        "# in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "# hidden_layer = 256\n",
        "# # and replace the mask predictor with a new one\n",
        "# model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
        "#                                                     hidden_layer,\n",
        "#                                                     num_classes)\n",
        "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# model=model.to(device)\n",
        "\n",
        "# params = [p for p in model.parameters() if p.requires_grad]\n",
        "# optimizer = torch.optim.SGD(params, lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WwzQQ-d-tqz"
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "\n",
        "num_classes = 2\n",
        "# load an instance segmentation model pre-trained pre-trained on COCO\n",
        "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# get number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "# replace the pre-trained head with a new one\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "# now get the number of input features for the mask classifier\n",
        "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "hidden_layer = 256\n",
        "# and replace the mask predictor with a new one\n",
        "model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
        "                                                    hidden_layer,\n",
        "                                                    num_classes)\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model=model.to(device)\n",
        "\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycQLrwkYLKbM"
      },
      "source": [
        "# sp = '/content/drive/MyDrive/warwick_qu_dataset_released_2016_07_08/Warwick QU Dataset (Released 2016_07_08)/trained_models'\n",
        "# model_names = os.listdir(os.path.join(sp))\n",
        "# model_names\n",
        "def latest_model():\n",
        "  sp = '/content/drive/MyDrive/warwick_qu_dataset_released_2016_07_08/Warwick QU Dataset (Released 2016_07_08)/trained_models'\n",
        "  largest = 0\n",
        "  model_names = os.listdir(os.path.join(sp))\n",
        "  for i in model_names:\n",
        "    nr = int(list(filter(str.isdigit, i))[0])\n",
        "    if nr>largest:\n",
        "      largest = nr\n",
        "  return largest\n",
        "\n",
        "latest_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I77iWmnM9IWL"
      },
      "source": [
        "# Load previously trained model \n",
        "model_nr = latest_model() \n",
        "save_path = '/content/drive/MyDrive/warwick_qu_dataset_released_2016_07_08/Warwick QU Dataset (Released 2016_07_08)/trained_models/model'+str(model_nr)\n",
        "model.load_state_dict(torch.load(save_path))\n",
        "# model=model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6ME2_rvGv_G"
      },
      "source": [
        "\n",
        "# Perform training loop for n epochs\n",
        "loss_list = []\n",
        "n_epochs = 10\n",
        "model.train()\n",
        "for epoch in tqdm(range(n_epochs)):\n",
        "    loss_epoch = []\n",
        "    iteration=1\n",
        "    for images,targets in tqdm(data_loader_train):\n",
        "\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        optimizer.zero_grad()\n",
        "        model=model.double()\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        losses.backward()       \n",
        "        optimizer.step()\n",
        "        # print('loss:', losses.item())\n",
        "        # loss_epoch.append(losses.item())\n",
        "        loss_epoch.append(losses.item())\n",
        "        # Plot loss every 10th iteration\n",
        "\n",
        "        plt.plot(list(range(iteration)), loss_epoch)\n",
        "        plt.xlabel(\"Iteration\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.show()\n",
        "        iteration+=1\n",
        "    loss_epoch_mean = np.mean(loss_epoch) \n",
        "    loss_list.append(loss_epoch_mean) \n",
        "    # loss_list.append(loss_epoch_mean)    \n",
        "    print(\"Average loss for epoch = {:.4f} \".format(loss_epoch_mean))\n",
        "\n",
        "model_nr = latest_model() + 1\n",
        "save_path = '/content/drive/MyDrive/warwick_qu_dataset_released_2016_07_08/Warwick QU Dataset (Released 2016_07_08)/trained_models/model'+str(model_nr)\n",
        "torch.save(model.state_dict(), save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgY_xCzPPotV"
      },
      "source": [
        "loss_list2 = [1.7433, 1.0864, 0.8294, 0.6757, 0.5676]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O26CUBH1Eob"
      },
      "source": [
        "# Plot training loss\n",
        "plt.plot(list(range(n_epochs)), loss_list, label='traning loss')\n",
        "plt.legend()\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44lATqQFQYbP"
      },
      "source": [
        "# Load previously trained model \n",
        "# model.load_state_dict(torch.load(save_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WSASuQiHqVv"
      },
      "source": [
        "root_test = '/content/drive/MyDrive/warwick_qu_dataset_released_2016_07_08/Warwick QU Dataset (Released 2016_07_08)/test'\n",
        "dataset_test = WarwickCellDataset(root_test, transforms=torchvision.transforms.ToTensor()) # get_transform(train=True)\n",
        "data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=2, shuffle=False,collate_fn=lambda x:list(zip(*x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHmnPjNRvN7O"
      },
      "source": [
        "# images, targets=next(iter(data_loader_test))\n",
        "# images = list(image.to(device) for image in images)\n",
        "# targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "# images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u39jLOULHK6d"
      },
      "source": [
        "# Look at some of the images and predicted bbox's after training\n",
        "\n",
        "images, targets=next(iter(data_loader_test))\n",
        "images = list(image.to(device) for image in images)\n",
        "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "model=model.double()\n",
        "model.eval()\n",
        "output = model(images)\n",
        "\n",
        "with torch.no_grad():\n",
        "    view(images, output, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVLrW3SwPAvY"
      },
      "source": [
        "print(len(output[0]['boxes']))\n",
        "print(len(output[0]['scores']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "040c6LLJPTeT"
      },
      "source": [
        "output[0]['boxes'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub5_6hFzM-uV"
      },
      "source": [
        "# torchvision.utils.make_grid(images[i])\n",
        "for i in range(2):\n",
        "  # out = output[i]['scores'].to('cpu')\n",
        "  # out = out.detach().numpy()\n",
        "  for j in range(len(output[i]['scores'])):\n",
        "    if j < 0.7:\n",
        "      output[i]['boxes'][j] = torch.Tensor([0,0,0,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH8jnAowLd0G"
      },
      "source": [
        "def view_mask(targets, output, n=2, cmap='Greys'):\n",
        "    figure = plt.figure(figsize=(15,10))\n",
        "    for i in range(n):\n",
        "      # plot target (true) masks\n",
        "      target_im = targets[i]['masks'][0].cpu().detach().numpy()\n",
        "      for k in range(len(targets[i]['masks'])):\n",
        "        target_im2 = targets[i]['masks'][k].cpu().detach().numpy()\n",
        "        target_im2[target_im2>0.5] = 1\n",
        "        target_im2[target_im2<0.5] = 0\n",
        "        target_im = target_im+target_im2\n",
        "\n",
        "      target_im[target_im>0.5] = 1\n",
        "      target_im[target_im<0.5] = 0\n",
        "      ax = figure.add_subplot(2,2, i+1)\n",
        "      ax.imshow(target_im, cmap=cmap)\n",
        "      # Plot output (predicted) masks\n",
        "      output_im = output[i]['masks'][0][0, :, :].cpu().detach().numpy()\n",
        "      for k in range(len(output[i]['masks'])):\n",
        "        output_im2 = output[i]['masks'][k][0, :, :].cpu().detach().numpy()\n",
        "        output_im2[output_im2>0.5] = 1\n",
        "        output_im2[output_im2<0.5] = 0\n",
        "        output_im = output_im+output_im2\n",
        "\n",
        "      output_im[output_im>0.5] = 1\n",
        "      output_im[output_im<0.5] = 0\n",
        "      ax = figure.add_subplot(2,2, i+3)\n",
        "      ax.imshow(output_im, cmap=cmap)\n",
        "\n",
        "view_mask(targets, output, n=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0Ux8zqE4aDa"
      },
      "source": [
        "def IoU(y_real, y_pred):\n",
        "  # Intersection over Union loss function\n",
        "  intersection = y_real*y_pred\n",
        "  #not_real = 1 - y_real\n",
        "  #union = y_real + (not_real*y_pred)\n",
        "  union = (y_real+y_pred)-(y_real*y_pred)\n",
        "  return np.sum(intersection)/np.sum(union)\n",
        "\n",
        "def dice_coef(y_real, y_pred, smooth=1):\n",
        "  intersection = y_real*y_pred\n",
        "  union = (y_real+y_pred)-(y_real*y_pred)\n",
        "  return np.mean((2*intersection+smooth)/(union+smooth))\n",
        "\n",
        "def confusion_matrix(y_true, y_pred):\n",
        "  y_true= y_true.flatten()\n",
        "  y_pred = y_pred.flatten()*2\n",
        "  cm = y_true+y_pred\n",
        "  cm = np.bincount(cm, minlength=4)\n",
        "  tn, fp, fn, tp = cm\n",
        "  return tp, fp, tn, fn\n",
        "\n",
        "def get_f1_score(y_true, y_pred):\n",
        "    \"\"\"Return f1 score covering edge cases\"\"\"\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred)\n",
        "    f1_score = (2 * tp) / ((2 * tp) + fp + fn)\n",
        "\n",
        "    return f1_score     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiOyPrbk5xkA"
      },
      "source": [
        "# Get IoU score for whole test set\n",
        "IoU_scores_list = []\n",
        "dice_coef_scores_list = []\n",
        "f1_scores_list = []\n",
        "skipped = 0\n",
        "for images,targets in tqdm(data_loader_test):\n",
        "  images = list(image.to(device) for image in images)\n",
        "  targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "  model=model.double()\n",
        "  model.eval()\n",
        "  output = model(images)\n",
        "  # print(targets)\n",
        "  target_im = targets[0]['masks'][0].cpu().detach().numpy()\n",
        "  for k in range(len(targets[0]['masks'])):\n",
        "    target_im2 = targets[0]['masks'][k].cpu().detach().numpy()\n",
        "    target_im2[target_im2>0.5] = 1\n",
        "    target_im2[target_im2<0.5] = 0\n",
        "    target_im = target_im+target_im2\n",
        "\n",
        "  target_im[target_im>0.5] = 1\n",
        "  target_im[target_im<0.5] = 0\n",
        "  target_im = target_im.astype('int64')\n",
        "  \n",
        "  # Plot output (predicted) masks\n",
        "  output_im = output[0]['masks'][0][0, :, :].cpu().detach().numpy()\n",
        "  for k in range(len(output[0]['masks'])):\n",
        "    output_im2 = output[0]['masks'][k][0, :, :].cpu().detach().numpy()\n",
        "    output_im2[output_im2>0.5] = 1\n",
        "    output_im2[output_im2<0.5] = 0\n",
        "    output_im = output_im+output_im2\n",
        "\n",
        "  output_im[output_im>0.5] = 1\n",
        "  output_im[output_im<0.5] = 0\n",
        "  output_im = output_im.astype('int64')\n",
        "\n",
        "  if target_im.shape != output_im.shape:\n",
        "    skipped+=1\n",
        "    continue\n",
        "  \n",
        "  dice_coef_score = dice_coef(y_real=target_im, y_pred=output_im)\n",
        "  dice_coef_scores_list.append(dice_coef_score)\n",
        "  IoU_score = IoU(y_real=target_im, y_pred=output_im) \n",
        "  IoU_scores_list.append(IoU_score)\n",
        "  f1_score = get_f1_score(target_im, output_im)\n",
        "  f1_scores_list.append(f1_score)\n",
        "\n",
        "print('mean IoU score for test set:', np.mean(IoU_scores_list))\n",
        "print('mean Dice Coefficient score for test set:', np.mean(dice_coef_scores_list))\n",
        "print('mean f1 score for test set:', np.mean(f1_scores_list))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1EO0dYa7uE6"
      },
      "source": [
        "print('mean IoU score for test set:', np.mean(IoU_scores_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAhbzmLb43dO"
      },
      "source": [
        "for i in range(2):\n",
        "  # plot target (true) masks\n",
        "  target_im = targets[i]['masks'][0].cpu().detach().numpy()\n",
        "  for k in range(len(targets[i]['masks'])):\n",
        "    target_im2 = targets[i]['masks'][k].cpu().detach().numpy()\n",
        "    target_im2[target_im2>0.5] = 1\n",
        "    target_im2[target_im2<0.5] = 0\n",
        "    target_im = target_im+target_im2\n",
        "\n",
        "  target_im[target_im>0.5] = 1\n",
        "  target_im[target_im<0.5] = 0\n",
        "  # Plot output (predicted) masks\n",
        "  output_im = output[i]['masks'][0][0, :, :].cpu().detach().numpy()\n",
        "  for k in range(len(output[i]['masks'])):\n",
        "    output_im2 = output[i]['masks'][k][0, :, :].cpu().detach().numpy()\n",
        "    output_im2[output_im2>0.5] = 1\n",
        "    output_im2[output_im2<0.5] = 0\n",
        "    output_im = output_im+output_im2\n",
        "\n",
        "  output_im[output_im>0.5] = 1\n",
        "  output_im[output_im<0.5] = 0\n",
        "\n",
        "IoU(y_real=target_im, y_pred=output_im) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmrsUtyvxiSr"
      },
      "source": [
        "len(output[0]['masks'])\n",
        "im = output[0]['masks'][0][0, :, :].cpu().detach().numpy()\n",
        "# im2 = outputs[0]['masks'][1][0, :, :].cpu().detach().numpy()\n",
        "it = 0\n",
        "for i in range(len(output[0]['masks'])):\n",
        "  im2 = output[0]['masks'][i][0, :, :].cpu().detach().numpy()\n",
        "  im2[im2>0.5] = 1\n",
        "  im2[im2<0.5] = 0\n",
        "  im = im+im2\n",
        "  it+=1\n",
        "print(it)\n",
        "# im_new = np.concatenate((im, im2)) \n",
        "im[im>0.5] = 1\n",
        "im[im<0.5] = 0\n",
        "plt.imshow(im, cmap='Greys')\n",
        "# outputs[0]['masks']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDvJ8hDt4g-Z"
      },
      "source": [
        "# im = outputs[0]['masks'][20][0, :, :].cpu().detach().numpy()\n",
        "# im[im>0.5] = 1\n",
        "# im[im<0.5] = 0\n",
        "# plt.imshow(im, cmap='Greys')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTzhIprUHe1S"
      },
      "source": [
        "# # outputs = model(images)\n",
        "# outputs = [{k: v.to(device) for k, v in t.items()} for t in output]\n",
        "# pred_score = list(outputs[0]['scores'].detach().numpy())\n",
        "# pred_t = [pred_score.index(x) for x in pred_score if x>=threshold][-1]\n",
        "# masks = (outputs[0]['masks']>0.5).squeeze().numpy()\n",
        "# masks = masks[:pred_t+1]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}